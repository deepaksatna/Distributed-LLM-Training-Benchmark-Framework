# Project Structure

This document explains the organization of the LLM Training Benchmark repository.

```
llm-training-benchmark/
â”‚
â”œâ”€â”€ README.md                          # Main documentation (START HERE)
â”œâ”€â”€ LICENSE                            # MIT License
â”œâ”€â”€ PROJECT_STRUCTURE.md               # This file
â”œâ”€â”€ .gitignore                         # Git ignore rules
â”‚
â”œâ”€â”€ docs/                              # Additional documentation
â”‚   â”œâ”€â”€ ARCHITECTURE.md                # System architecture deep-dive
â”‚   â””â”€â”€ TROUBLESHOOTING.md             # Common issues and solutions
â”‚
â”œâ”€â”€ benchmarking/                      # Core training code
â”‚   â””â”€â”€ train_harness.py               # Main training script (700+ lines)
â”‚                                      # - TinyGPT model definition
â”‚                                      # - Training loop with metrics
â”‚                                      # - DDP/FSDP/ZeRO wrapper logic
â”‚                                      # - Results export to JSON
â”‚
â”œâ”€â”€ configs/                           # Strategy configurations
â”‚   â”œâ”€â”€ deepspeed/
â”‚   â”‚   â”œâ”€â”€ zero2.json                 # DeepSpeed ZeRO-2 config
â”‚   â”‚   â””â”€â”€ zero3.json                 # DeepSpeed ZeRO-3 config
â”‚   â””â”€â”€ fsdp/
â”‚       â””â”€â”€ fsdp_config.yaml           # FSDP sharding strategy config
â”‚
â”œâ”€â”€ docker/                            # Container build files
â”‚   â”œâ”€â”€ Dockerfile                     # Multi-stage Docker build
â”‚   â”‚                                  # - Base: CUDA 12.1 + cuDNN 8
â”‚   â”‚                                  # - PyTorch 2.1.0 + DeepSpeed
â”‚   â”‚                                  # - All dependencies offline
â”‚   â””â”€â”€ entrypoint.sh                  # Container startup script
â”‚                                      # - Computes RANK from K8s index
â”‚                                      # - Sets MASTER_ADDR correctly
â”‚                                      # - Launches train_harness.py
â”‚
â”œâ”€â”€ k8s/                               # Kubernetes manifests
â”‚   â”œâ”€â”€ namespace.yaml                 # Creates "bench" namespace
â”‚   â”œâ”€â”€ serviceaccount.yaml            # RBAC for pods
â”‚   â”œâ”€â”€ service-master.yaml            # ClusterIP for master discovery
â”‚   â”œâ”€â”€ job-master.template.yaml       # Master (rank 0) job template
â”‚   â”œâ”€â”€ job-workers.template.yaml      # Workers (rank 1..N) indexed job
â”‚   â”œâ”€â”€ job-single.tmpl.yaml           # Generic single job template
â”‚   â””â”€â”€ job-smoke-1gpu.yaml            # Single-GPU smoke test
â”‚
â”œâ”€â”€ scripts/                           # Automation scripts
â”‚   â”‚
â”‚   â”œâ”€â”€ build.sh                       # Build Docker image locally
â”‚   â”œâ”€â”€ push.sh                        # Push image to OCIR
â”‚   â”œâ”€â”€ verify_offline.sh              # Verify no external downloads
â”‚   â”‚
â”‚   â”œâ”€â”€ launch_smoke.sh                # Run 1-GPU smoke test
â”‚   â”œâ”€â”€ launch_multi.sh                # Launch distributed job (master + workers)
â”‚   â”œâ”€â”€ run_all_benchmarks.sh          # ðŸš€ MAIN SCRIPT - Run all 8 benchmarks
â”‚   â”‚                                  # - Sequentially runs all configs
â”‚   â”‚                                  # - Collects results after each
â”‚   â”‚                                  # - Generates analysis at end
â”‚   â”‚
â”‚   â”œâ”€â”€ collect_results.sh             # Extract results from pod logs
â”‚   â”œâ”€â”€ install_analysis_deps.sh       # Install pandas, matplotlib, numpy
â”‚   â”‚
â”‚   â”œâ”€â”€ parse_metrics.py               # JSON â†’ CSV aggregation
â”‚   â”œâ”€â”€ plot.py                        # Generate performance plots
â”‚   â”œâ”€â”€ make_report.py                 # Generate markdown report
â”‚   â”‚
â”‚   â””â”€â”€ check_cluster_gpus.sh          # Verify GPU availability on nodes
â”‚
â”œâ”€â”€ images/                            # Performance visualizations
â”‚   â”œâ”€â”€ tokens_per_sec_vs_gpu.png      # Throughput comparison
â”‚   â”œâ”€â”€ step_time_vs_gpu.png           # Latency comparison
â”‚   â”œâ”€â”€ scaling_efficiency.png         # Scaling analysis
â”‚   â””â”€â”€ gbps_vs_gpu.png                # Data transfer rates
â”‚
â””â”€â”€ results/                           # Benchmark outputs (gitignored)
    â””â”€â”€ example_output/
        â””â”€â”€ README.md                  # Example results documentation

    # After running benchmarks, structure will be:
    # results/
    # â”œâ”€â”€ bench-master-ddp-ws2-seq2048.log
    # â”œâ”€â”€ bench-master-ddp-ws2-seq2048_results/
    # â”‚   â””â”€â”€ result.json
    # â”œâ”€â”€ bench-master-ddp-ws4-seq2048.log
    # â”œâ”€â”€ bench-master-ddp-ws4-seq2048_results/
    # â”‚   â””â”€â”€ result.json
    # â”œâ”€â”€ ... (8 configurations total)
    # â””â”€â”€ summary/
    #     â”œâ”€â”€ metrics.csv
    #     â”œâ”€â”€ BENCHMARK_REPORT.md
    #     â””â”€â”€ plots/
    #         â”œâ”€â”€ tokens_per_sec_vs_gpu.png
    #         â”œâ”€â”€ step_time_vs_gpu.png
    #         â”œâ”€â”€ scaling_efficiency.png
    #         â””â”€â”€ gbps_vs_gpu.png
```

## File Dependencies

### Build Phase
```
docker/Dockerfile
â”œâ”€â”€ References: benchmarking/train_harness.py
â”œâ”€â”€ References: configs/**/*.{json,yaml}
â”œâ”€â”€ References: docker/entrypoint.sh
â””â”€â”€ Produces: Docker image â†’ OCIR
```

### Deploy Phase
```
k8s/namespace.yaml         (must exist)
k8s/serviceaccount.yaml    (must exist)
k8s/service-master.yaml    (must exist for multi-GPU)
â”‚
scripts/launch_multi.sh
â”œâ”€â”€ Reads: k8s/job-master.template.yaml
â”œâ”€â”€ Reads: k8s/job-workers.template.yaml
â”œâ”€â”€ Substitutes: IMAGE, WORLD_SIZE, STRATEGY, etc.
â””â”€â”€ Creates: K8s jobs in cluster
```

### Training Phase
```
Pod starts
â”‚
â”œâ”€â”€ docker/entrypoint.sh
â”‚   â”œâ”€â”€ Computes RANK from JOB_COMPLETION_INDEX
â”‚   â”œâ”€â”€ Sets environment variables
â”‚   â””â”€â”€ Executes: python3 benchmarking/train_harness.py --args...
â”‚
â””â”€â”€ benchmarking/train_harness.py
    â”œâ”€â”€ Loads configs/deepspeed/*.json (if ZeRO)
    â”œâ”€â”€ Loads configs/fsdp/*.yaml (if FSDP)
    â”œâ”€â”€ Trains model
    â””â”€â”€ Outputs JSON to stdout
```

### Collection Phase
```
scripts/collect_results.sh
â”œâ”€â”€ Input: Namespace, Job name
â”œâ”€â”€ Executes: kubectl logs <pod>
â”œâ”€â”€ Extracts: JSON between markers
â””â”€â”€ Writes: results/<job-name>_results/result.json
```

### Analysis Phase
```
scripts/parse_metrics.py
â”œâ”€â”€ Input: results/ directory
â”œâ”€â”€ Finds: All result.json files
â”œâ”€â”€ Aggregates: Into pandas DataFrame
â””â”€â”€ Writes: results/summary/metrics.csv

scripts/plot.py
â”œâ”€â”€ Input: metrics.csv
â”œâ”€â”€ Generates: 4 matplotlib plots
â””â”€â”€ Writes: results/summary/plots/*.png

scripts/make_report.py
â”œâ”€â”€ Input: metrics.csv
â”œâ”€â”€ Generates: Markdown tables and analysis
â””â”€â”€ Writes: results/summary/BENCHMARK_REPORT.md
```

## Key Files Explained

### train_harness.py (700+ lines)
**Purpose:** Core training script with model, strategies, and metrics.

**Sections:**
1. **Imports & Setup** (lines 1-50)
   - PyTorch, DeepSpeed, FSDP, NCCL
   - Argument parsing

2. **TinyGPT Model** (lines 51-150)
   - GPT-2 architecture
   - 117M parameters
   - Configurable layers, heads, embedding size

3. **Strategy Wrappers** (lines 151-280)
   - `wrap_model()` function
   - DDP, FSDP, ZeRO-2, ZeRO-3 logic
   - Fixed DeepSpeed config handling

4. **Training Loop** (lines 281-450)
   - Synthetic data generation
   - Forward/backward passes
   - Metrics collection (VRAM, timing, throughput)

5. **Results Export** (lines 451-500)
   - JSON formatting
   - Stdout output with markers
   - File save (for local runs)

6. **Main Entry Point** (lines 501-700)
   - Process group initialization
   - Argument validation
   - Strategy selection
   - Training execution

### entrypoint.sh
**Purpose:** Compute correct environment for distributed training.

**Critical Logic:**
```bash
# Compute RANK from Kubernetes indexed job
if [ -n "${JOB_COMPLETION_INDEX:-}" ]; then
  export RANK=$((JOB_COMPLETION_INDEX + 1))
fi

# Master uses its own IP, workers use service DNS
if [ "$RANK" = "0" ] && [ -n "${POD_IP:-}" ]; then
  export MASTER_ADDR="$POD_IP"
else
  export MASTER_ADDR="${MASTER_ADDR:-127.0.0.1}"
fi
```

### run_all_benchmarks.sh
**Purpose:** Orchestrate complete benchmark suite.

**Workflow:**
1. Define 8 configurations (DDP/FSDP/ZeRO-2/ZeRO-3 Ã— 2/4 GPUs)
2. For each config:
   - Launch jobs via `launch_multi.sh`
   - Wait for completion (max 15 min)
   - Collect results via `collect_results.sh`
   - Delete jobs for cleanup
3. After all benchmarks:
   - Parse to CSV
   - Generate plots
   - Create report

### collect_results.sh
**Purpose:** Extract JSON from logs even after pod termination.

**Key Innovation:**
```bash
# Works even after pod terminates!
kubectl logs $POD_NAME > log.txt

# Extract JSON between markers
sed -n '/BENCHMARK_RESULT_JSON_START/,/BENCHMARK_RESULT_JSON_END/p' log.txt | \
  sed '1d;$d' > result.json
```

## Configuration Flow

### Strategy Selection
```
User runs:
./scripts/run_all_benchmarks.sh

â””â”€â”€â–¶ For each benchmark:
     ./scripts/launch_multi.sh --strategy ddp --world-size 2 ...

     â””â”€â”€â–¶ Creates job with env:
          STRATEGY=ddp
          WORLD_SIZE=2

          â””â”€â”€â–¶ entrypoint.sh passes to:
               python3 train_harness.py --strategy ddp --world-size 2

               â””â”€â”€â–¶ train_harness.py:
                    if args.strategy == "ddp":
                        model = DDP(model, ...)
                    elif args.strategy == "fsdp":
                        model = FSDP(model, ...)
                    elif args.strategy in ["zero2", "zero3"]:
                        model, opt = deepspeed.initialize(...)
```

### Config File Loading
```
DeepSpeed ZeRO-2:
args.strategy = "zero2"
args.deepspeed_config = "/app/configs/deepspeed/zero2.json"
â””â”€â”€â–¶ train_harness.py loads and modifies JSON
     â””â”€â”€â–¶ Sets batch sizes as integers (critical fix!)

DeepSpeed ZeRO-3:
args.strategy = "zero3"
args.deepspeed_config = "/app/configs/deepspeed/zero3.json"
â””â”€â”€â–¶ Same process

FSDP:
Uses in-code configuration (transformer_auto_wrap_policy)
Optional: Can load from configs/fsdp/fsdp_config.yaml
```

## Where to Start

### For Running Benchmarks:
1. Read: `README.md` (quick start section)
2. Edit: `scripts/push.sh` (your OCIR details)
3. Run: `./scripts/build.sh && ./scripts/push.sh`
4. Run: `./scripts/run_all_benchmarks.sh`

### For Understanding Implementation:
1. Read: `docs/ARCHITECTURE.md`
2. Read: `benchmarking/train_harness.py`
3. Read: `docker/entrypoint.sh`
4. Read: `scripts/run_all_benchmarks.sh`

### For Debugging Issues:
1. Read: `docs/TROUBLESHOOTING.md`
2. Check: Pod logs via `kubectl logs`
3. Verify: Environment variables in `entrypoint.sh`

### For Modifying Strategies:
1. Edit: `benchmarking/train_harness.py` (wrap_model function)
2. Edit: `configs/deepspeed/*.json` (ZeRO configs)
3. Rebuild: `./scripts/build.sh && ./scripts/push.sh`

### For Adding New GPUs (A100, H100):
1. Read: `README.md` (Supported GPU Platforms section)
2. Edit: Batch sizes, sequence lengths in configs
3. Test: Run smoke test first `./scripts/launch_smoke.sh`

## Quick Reference

| Task | File | Command |
|------|------|---------|
| Build image | `scripts/build.sh` | `./scripts/build.sh` |
| Push to OCIR | `scripts/push.sh` | `./scripts/push.sh` |
| Run all benchmarks | `scripts/run_all_benchmarks.sh` | `./scripts/run_all_benchmarks.sh` |
| Run smoke test | `scripts/launch_smoke.sh` | `./scripts/launch_smoke.sh` |
| Test single config | `scripts/launch_multi.sh` | `./scripts/launch_multi.sh --strategy ddp --world-size 2` |
| Collect results | `scripts/collect_results.sh` | `./scripts/collect_results.sh bench job-name ./results` |
| Generate plots | `scripts/plot.py` | `python3 scripts/plot.py --results metrics.csv --out plots/` |
| View logs | - | `kubectl logs <pod> -n bench` |
| Debug pod | - | `kubectl exec -it <pod> -n bench -- bash` |

---

**Last Updated:** January 6, 2026
**Maintained By:** Oracle AI CoE
